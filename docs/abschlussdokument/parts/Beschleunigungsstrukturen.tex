\section{Beschleunigungsstrukturen}

Ein Ziel userer Implementierung war es, jederzeit die Möglichkeit zu haben, verschiedene Beschleunigungsstrukturen vergleichen zu können, ohne große Einbusen bei der Effizienz des Codes zu haben. Deshalb geben wir die jeweils zu benutzende Datenstruktur über einen template-Parameter in die entsprechenden Funktionen. Die Datenstrukturen teilen sich eine einfache Schnittstelle: Es existieren die Methoden \code{build} und \code{trace}.

\subsection{Naive Szenentraversierung}

Die \code{DummyAcceleration} beinhaltet, wie der Name schon andeutet, keinen Algorithmus zur Beschleunigung. Sie implementiert die Schnittstelle und führt eine einfache lineare Iteration über alle Dreiecke durch, um den korrekten Schnittpunkt zu finden.

\subsection{BIH}
\label{ssec:bih}

Die Binary Interval Hierarchy (BIH) ist die erste Datenstruktur, die wir für unseren Renderer implementiert haben. Sie zeichnet sich durch eine einfache Implementierung aus und kombiniert Eigenschaften von KD-Bäumen und Bounding Volume Hierarchies (BVH). Außerdem ist sie verglichen mit einer BVH sehr speichereffizient, und der maximale Speicherverbrauch lässt sich aus der Anzahl der Dreiecke vorberechnen.

Die BIH unterteilt die Menge aller Dreiecke rekursiv in einen binären Baum. Ein innerer Knoten teilt die Menge seiner Kind-Dreiecke in eine negative (links) und positive (rechts) Seite entlang einer der drei Raumachsen. Durch zwei Ebenen (die jeweils orthogonal zur entsprechenden Unterteilungsachse liegen, deshalb genügt ein je \code{float} Wert um sie zu definieren) wird angegeben, wo sich der maximale bzw. minimale Eckpunkt aller Dreiecke der linken bzw. rechten Hälfte befindet. Anders gesagt wird jeweils eine Intervallgrenze angegeben, innerhalb derer sich die Kind-Dreiecke befinden.

Durch rekursives Absteigen durch die BIH kann ein achsenausgerichteter Hüllwürfel für je eine der Dreiecksmengen angegeben werden, indem sukzessiv das Maximum bzw. Minimun des Hüllwürfels mit den entsprechenden Koordinaten der Ebenen ersetzt werden. Dies ist eine Annäherung an eine BVH mit achsenausgerichteten Hüllwürfeln.

Im ursprünglichen Paper benutzt der Unterteilungsalgorithmus keine Surface Area Heuristic (SAH), sondern baut darauf, dass durch die Intervallangabe geometreifreier Raum als solcher erkannt wird. Auch die nächste Schnittebene wir aus dem sukzessive verkleinerten Hüllwürfel aller Dreiecke berechnet: Es wird entlang der längsten Ache in der Mitte geschnitten. Dann werden die Kind-Dreiecke anhand der Schnittebene und ihres Schwerpunkts in eine linke und rechte Hälfte unterteilt. Gleichzeitig werden die Koordinaten der linken und rechten Ebene für den Knoten berechnet. Dieser Schritt wird rekursiv für die linke und rechte Teilmenge durchgeführt bis eine bestimmte Anzahl von Dreiecken unterschritten wird oder in der Unterteilung eine bestimmte Zahl oft eine der beiden Kind-Mengen leer war (die Unterteilung ist fehlgeschlafen). Dann wird ein Blattknoten erstellt, der eine variable Anzahl von Kind-Dreiecken haben kann.

Der Unterteilungsalgorithmus muss keine die Unterteilungsebene überlappenden Dreiecke behandeln und berechnet den Unterteilungspunkt in konstanter Zeit. Dadurch liegt die Komplexität in $O(n \log n)$, was das theoretische Minimum ist (vgl. mit Quicksort). Da weiterhin während der Berechnung kein Speicher dynamisch erzeugt werden muss (die maximale Anzahl der Knoten lässt sich im Vorhinein bestimmen), ist der Algorithmus sehr effizient und eine Parallelisierung lohnt sich nur bei großen Szenen. Tatsächlich trägt der Vorbereitungsschritt, wo der Hüllwürfel aller Dreiecke und Schwerpunkt und Hüllwürfel jedes Dreiecks vorberechnet werden, einen großen Teil zur Gesamtlaufzeit bei (auf einer Maschine waren es $1/3$ der Ausführungszeit). TODO Wollen wir mal gucken wie viel schneller das wird mit OpenMP?

Die kompakte Speicherung der Knoten und Blätter der BIH sind erwähnenswert. Beide werden in einem \code{union} zusammengefasst, damit sie in einen \code{std::vector} gespeichert werden können. Wie im Ursprungspaper vorgeschlagen, wird der Typ des Knoten (Blatt, Unterteilung in X/Y/Z) und die Referenz auf die Kinder in einer 32-Bit Zahl gespeichert. Für Unterteilungsknoten (innere Knoten), gibt die Kindreferenz ein Element im selben Vektor an, Blattknoten zeigen in die Liste der Dreiecke. Dadurch beiben 30 Bit für die Referenzierung von Dreiecken übrig, also werden maximal $2^{30}$ Dreiecke unterstützt, was aber ausreichend ist für die gegebenen Problemgrößen.

Die Traversierung der BIH ist grunsätzlich gleich der Traversierung einer BVH: Für jeden inneren Knoten wird ein Schnittest des Strahls mit beiden Kindknoten berechnet. Im Fall der BIH wird der akkumulierte Hüllwürfel benutzt. Da sich beide Kinder grundsätzlich überlappen können, müssen immer beide Kinder betrachtet werden. Dadurch wird nur "`culling"' berechnet, also konservativ alle Dreiecke verworfen, die den Strahl in keinem Fall schneiden. Da die BIH aber achsenausgerichtete Hüllwürfel als konservative Annäherung an die Geometrie hat, kann folgende Optimierung durchgeführt werden: Die beiden Kind-Knoten werden in der Reihenfolge verglichen, in die sich der Strahl für die entsprechende Koordinate fortbewegt. Wird ein Schnittpunkt gefunden muss das zweite Kind nur dann betreten werden, wenn der Schnittpunkt des Strahls mit dem Hüllwürfel näher ist als der gefundene Dreiecksschnitt.

Die Traversierungsalgorithmus ist iterativ implementiert. Der notwendige Stack liegt im Stack der Programmausführung. Das hat die Effizienz der Implementierung verbessert.

\subsection{KD-Baum}
\label{ssec:kdtree}

Der KD-Baum entstand aus der Erkenntniss, dass Path-Tracing deutlich mehr Schnitttests pro Pixel berechnet und sich deshalb eine effizientere Datenstruktur auf Kosten eines größeren Aufwands beim Bauen lohnt. Da die BIH in der Struktur der Knoten und in der Traversierung ähnlich eines KD-Baums ist, ist die Implementierung direkt aus der vorhandenen Implementierung der BIH entstanden.

Der Unterschied zwischen beiden Datenstrukturen liegt darin, dass die inneren Knoten eines KD-Baums nur die Unterteilingsebene speichern und sich die linke und rechte Dreiecksmenge grundsätzlich nicht überlappen darf. In vielen anderen Implementierungen werden dazu die Dreiecke beim Aufbau an der Unterteilungsebene abgeschnitten (geclippt), in unserer Implementierung markieren wir überlappende Dreiecke und behandeln sie gesondert in der Traversierung (siehe unten). Da der KD-Baum in seiner einfachen Form keine explizite Möglichkeit hat, geometriefreien Raum auszudrücken, ist hier eine gute Wahl der Unterteilungsebenen wichtig (leerer Raum wird dadurch optimiert, dass möglicht schnell ein Blatt erreicht wird). Für die Wahl der Unterteilungsebenen wird eine Surface Area Heuristic (SAH) minimiert. Der naive Algorithmus hat dafür eine Komplexität von $O(n^2)$, eine Optimierung aus dem Paper berechnet dies in $O(n \log^2 n)$. Die $O(n \log n)$ Variante, die ebenfalls beschrieben wird, haben wir nicht implementiert, da eine Parallelisierung wahrscheinlich schwer geworden wäre und die parallele $O(n \log^2 n)$ Variante für unsere Problemgrößen ausreichend schnell war.

Da beim Aufbau überlappende Dreiecke dupliziert werden müssen, das im Vorhinein nicht abgeschätzt werden kann und so eine Implementierung mit \code{std::vector} zur Beschreibung einer Menge von Dreiecken schwierig ist, ist unsere Implementierung duch die Nutzung von \code{std::list} eher ineffizient. Hier lohnt sich deshalb eine Parallelisierung des Aufbaus, wie in \ref{ssec:parallel-accel-build} beschrieben wird.

Die Traversierung eines KD-Baums ist einfacher verglichen mit der BIH: Da sich Nachbarknoten nicht überlappen können, kann nach einem gefundenen Dreiecksschnitt in einem Blatt die Traversierung beendet werden. Da wir aber überlappende Dreiecke nicht abscheiden gilt dies nur für Dreiecke, die in Eltern-Knoten die Unterteilungsebene nicht überlappen. Diese Behandlung erschien uns einfacher und weniger fehleranfällig (und sie liefert genauere Ergebnisse) als Abschneiden von Dreiecken im Aufbau.

\subsection{Umsortieren von Dreiecken in der Szene}

In der ursprünglichen Implementierung zeigte ein Blatt auf eine Liste von \code{TriangleIndex}, aus derer die korrekten Dreiecke in der Szene referenziert werden. Die Idee war hier, dass eine Beschleunigungsstruktur die Szene nich verändern sollte. Allerdings bedeutet dies eine zusätzliche Indirektion beim Auslesen der Dreiecke eines Blattes und ein unregelmäßiges Zugriffsmuster auf den Speicher, was die Cache-Effizienz verringert. Dieser Effekt war in einem Sampling-Profier erkennbar. Die Indirektion konnte mit wenig Aufwand entfernt werden, indem nach dem Aufbau der Datenstruktur die Dreiecke in der Szene anhand der Liste von \code{TriangleIndex} umsortiert werden.

\subsection{Paralleler Aufbau}
\label{ssec:parallel-accel-build}

Die jeweiligen Algorithmen, die unsere beiden Datenstrukturen aufbauen, haben eine ähnliche Struktur: Begonnen wird mir dem Hüllwürfel der gesamten Szene und der Menge aller Dreiecke, dann wird rekursiv in zwei Teilmengen unterteilt oder ein Blatt erstellt. In jedem Rekursionsschritt werden neue Knoten der Liste aller Knoten hinzugefügt. Die rekursiven Funktionsaufrufe haben keine Rückgabewerte und die Mengen, die durch die Funktionsparameter aufgespannt werden, überlappen sich nicht.

Grundsätzlich ist die Parallelisierung von rekursiven Algorithmen nicht trivial. Im Vorlesungsteil haben wir OpenMP Tasks als eine Möglichkeit kennen gelernt, Rekursion zu parallelisieren. Allerdings haben wir damals schon festgestellt, dass diese Technik einen nicht zu vernachlässigenden Overhead hat und Tuning-Parameter angegeben werden müssen, um nicht zu viele Tasks zu erstellen. Außerdem kann ein OpenMP Task erst beendet werden, wenn alle Kind-Tasks abgeschlossen sind.

Wir haben in unserem Projekt eine alternative Implementierung von paralleler Rekursion gebaut. Die Grundidee ist, dass wir möglichst oft versuchen, einen rekursiven Funktionsaufruf parallel durchzuführen. Dieser wird aber nur dann auf einen anderen Ausführungsfaden übertragen, wenn ein Faden vorhanden ist, der gerade keine Arbeit verrichtet. Die Überlegung ist, dass wenn schnell festgestellt werden kann, dass kein freier Faden zur Verfügung steht, wir keine Zeit verloren haben und nur wenn Parallelisierung möglich ist der Aufwand für die Übertragung in Kauf genommen wird. Außerdem soll es keinen ausgewiesenen Faden geben, der für die Aufgabenverteilung zuständig ist, sondern die Fäden sollen sich untereinander die Aufgaben zuteilen, um keinen Flaschenhals zu erzeugen.

Die Implementierung basiert auf dem \code{ThreadPool} und steht in der Datei \code{parallel\_worker.cpp}. In diesem werden bei Programmstart einige System-Threads erstellt, die mithilfe einer \code{std::condition\_variable} darauf warten, von einem anderen Faden aufgeweckt zu werden. Werden sie aufgeweckt, führen sie ein Arbeitspaket aus, das aus einer Funktionsreferenz (\code{std::function<void(void*)>}, alternativ könnte auch \code{void(*)(void*)} genutzt werden) und einem undefinierten Argument (\code{void*}) besteht. Nach Abarbeitung legen sich die Fäden so lange schlafen und warten auf das nächste Paket, bis sie aufgeweckt werden und sich bei gesetzter \code{interrupt}-Variable beenden.

Die Fäden tragen sich mit ihrem Index in einer nichtblockierenden einfach-verketteten Liste ein, wenn sie ein Arbeitspaket abgearbeitet haben. Dadurch kann mit einer einzigen Operation überprüft werden, ob ein freier Faden vorhanden ist (Lesen des Listenkopfes), und die Fäden synchronisieren sich nicht an der Liste.

Die Implementierung der paralellen Rekursion ist in \code{ParallelRecursion} (ebenfalls in \code{parallel\_worker.cpp}). Hauptaufgabe der Klasse ist es, die Funktionsargumente (template Parameter) zwischenzuspeichern wenn ein paralleler Aufruf auf einem freien Faden möglich ist.

Die theoretische Überlegung war, dass wir duch diese Implementierung parallele Rekursion ohne zusätzlichen Overhead bekommen, da die Überprüfung, ob Parallelität möglich ist, sehr schnell erfolgen kann. In der Praxis ist die Effizienz der Implementierung aber stark von der Anzahl der Fäden im \code{ThreadPool} abhängig (die ist auf den getesteten System gleich der Anzahl an unterstützten Hardware-Fäden) und wahrscheinlich von der Speicheranbindung der Prozessoren. Während auf einem 2-Kern Intel Atom 1,6 GHz (Netbook) ein Speedup von 4/3 beim Aufbau der BIH erziehlt wurde, trat weder eine Verbesserung noch eine Verschlechterung auf einem 8-Kern Intel Core i7 3,2 Ghz (Tower) ein. Für den ineffizienteren Code des KD-Baum Aufbaus lohnt sich dagegen die Parallelisierung auf dem 8-Kerner, hier beträgt der Speedup ca. 3.44. TODO was ist mit i41pc205, also 24 Kernen und NUMA(?) Speicher?

Ein Experiment, den \code{ThreadPool} zu nutzen um eine eigene parallele for-Schleife zu implementieren scheiterte, die Ausführung war mindestens um den Faktor 2 langsamer als die OpenMP Implementierung.

\subsection{Schnittberechnung mit \code{SSERay}}
\label{ssec:accel-intersect-sseray}

Da ein \code{SSERay} ein Bündel von Strahen repräsentiert, muss garantiert werden, dass für jeden einzelnen Strahl alle erforderlichen Kind-Knoten besucht werden. Dazu müssen die Traversierungsalgorithmen dahingehend erweiter werden, dass ein Kind-Knoten besucht wird wenn mindestens ein Strahl im Bündel den Hüllwürfel schneidet. Problematisch sind die Optimierungen, die die Traversierungen der Datenstrukturen abbrechen wenn kein näherer Schnittpunkt mehr gefunden werden kann, also nicht nur "`culling"' berechnet wird. Da dabei das Vorzeichen der Ausbreitungsrichtung des Strahl wichtig ist können sie nur korrekt sein, wenn alle Strahlen im Bündel das selbe Vorzeichen haben. Ist das nicht gegeben, degradieren die Beschleunigungsstrukturen wieder zu einfachen "`culling"'-Strukturen. Dieser Fall kann nur für einen kleinen Anteil von Pixelblöcken eintreten, der bei größeren Auflösungen weiter verkleinert wird (der Anteilige Flächeninhalt von problematischen Pixelblöcken wird kleiner bei größeren Auflösungen).

Die größte Beschleunigung des Schnitttests mit \code{SSERay} liegt darin, dass durch die gleichzeitige Berechnung von mehreren Strahlen der Aufwand der Traversierung durch die Zahl der Strahlen im Paket amortisiert wird. Vorraussetzung dafür ist, dass die Strahlen möglichst konvergent, also in eine ähnliche Richtung zeigen und wenig Streuung haben. Das ist bei Whitted-Style Tracing der Fall: Da wir Pixel-Blöcke verschießen haben die Strahlen den selben Ursprung und eine ähnliche Richtung. Schattentests werden immer mit dem selben Licht für alle Strahlen durchgeführt, deshalb konvergieren die Strahlen in der Position des Lichts. Bei Path-Tracing ist dieser Schritt für nicht-Primärstrahlen nicht ohne weiteres möglich. Allerdings ist das gröbere Rauschen immer noch ansehbar, wenn alle Strahlen im Paket in die selbe selbe Richtung diffus reflektiert werden, und die Berechnug ist deutlich schneller als echte zufällige Richtungen (\code{WITH\_CHEATS}). Aber auch im schlimmsten Fall, wenn alle Strahlen divergent sind, werden nie mehr Schnitttests mit der Datenstruktur berechnet als wenn einzelne Strahlen getestet werden würden.
